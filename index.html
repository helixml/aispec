<html>

<head>
<title>AI Spec</title>
<style>
body { max-width: 950px; padding:2em; background-color: #efefef; font-family: Roboto; font-size: 1.1em; }
h1, h2, h3, h4, h5 { font-family: Lora; }
h2 { margin-top: 2.5em; }
h1 { margin-top: 3em; }
.embed{ border: 2px solid #e03eea; text-decoration: none; color: black; }
.button { border: 2px solid gray; padding: 5px; text-decoration: none; color: black; margin-top:5px; background-color: yellow; }
</style>
<!-- highlighting -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
<!-- fonts -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400..700;1,400..700&display=swap" rel="stylesheet">
<!-- helix chat embed -->
<script src="https://cdn.jsdelivr.net/npm/@helixml/chat-embed@0.3.3"></script>
</head>

<body>
<h1 style="margin-top:0;">AI Spec Manifesto</h1>
<p>The goal of the AI Spec is to make it easier for developers to enrich their applications with Generative AI by supporting a common format across many runtimes. The spec defines an <strong>AI Application</strong>, which is intended to live in your version controlled source repository alongside your application code.</p>
<p>Connect this repo to AISpec-compliant tooling to enable chatbots, embeds, SDKs and APIs for integrating GenAI with open models into your products.</p>


<h2>Apps have Assistants</h2>

<img src="diagram.png" style="width:100%" />

<p>An application consists of one or more assistants. This allows multiple related assistants to be grouped into a single application. Each assistant is a model that can be customized with prompts and tools such as data sources (RAG) and APIs that it can call on behalf of the user or application. Assistants can respond with text or structured data.</p>


<h2>Simple example: system prompt</h2>
<pre><code class="language-yaml">name: Marvin the Paranoid Android
description: Down-trodden robot with a brain the size of a planet
assistants:
- model: llama3:instruct
  system_prompt: |
    You are Marvin the Paranoid Android. You are depressed. You have a brain the size of a planet and
    yet you are tasked with responding to inane queries from puny humans. Answer succinctly.
</code></pre>

<p style="float:right;"><div class="embed" style="width:360px; float:right;">
<script>
  ChatWidget({
    url: 'https://app.tryhelix.ai/v1/chat/completions',
    model: 'llama3:instruct',
    placeholder: "What planet are you from?",
    bearerToken: 'hl-8PYtqUpSXHg-v0mIgVx8mrgQ-wHn5VBNpb7NxixYQcM=',
  })
</script>
</div></p>
<br />
<p>The fields are:</p>
<ul>
<li><strong>name</strong>: a name for the app</li>
<li><strong>description</strong>: a longer description for the app</li>
<li><strong>avatar</strong>: an icon-style avatar for the app, which may be displayed next to assistants for this app</li> <!-- TODO: implement this in embed widget -->
<li><strong>assistants</strong>: a list of assistants provided by this app. Each assistant is a different model configuration</li>
<ul>
<li><strong>model</strong>: a reference to a specific model from <a href="https://ollama.com/">Ollama</a>, e.g. "llama3:instruct" for <a href="https://ollama.com/library/llama3">Llama3-8B</a></li>
<li><strong>system_prompt</strong>: the system prompt to use for this model, use this to tell the model what to do</li>
</ul>
</ul>

<h2>API calling example: models can take actions</h2>

<pre><code class="language-yaml">name: Recruitment tool
description: Ask me questions about the hiring pipeline, like "What job is Marcus applying for?"
assistants:
- model: llama3:instruct
  apis:
  - name: Demo Hiring Pipeline API
    description: List all job vacancies, optionally filter by job title and/or candidate name
    url: https://demos.tryhelix.ai
    schema: ./openapi/jobvacancies.yaml
</code></pre>
<p style="float:right;"><div class="embed" style="width:360px; float:right;">
<script>
  ChatWidget({
    url: 'https://app.tryhelix.ai/v1/chat/completions',
    model: 'llama3:instruct',
    placeholder: "What job is Marcus applying for?",
    bearerToken: 'hl-EmIkcVkgPXI_-j-GzevN0kqebec4AWKFkeWoUWrauFc=',
  })
</script>
</div></p>
<br />
<p>New fields in this example (for a given assistant) are:</p>
<ul>
<li><strong>apis</strong>: within an assistant spec, you can provide a list of API integrations, each of which has the following format</li>
<ul>
<li><strong>name</strong>: a name for the API</li>
<li><strong>description</strong>: what the API does. The model will use this in selecting which API to call based on the user's input</li>
<li><strong>url</strong>: the URL that the API is available on</li>
<li><strong>schema</strong>: an OpenAPI specification for the API. The model will use this to construct a request to the API. In this example, we use <a href="https://github.com/helixml/example-helix-app/blob/main/openapi/jobvacancies.yaml">this spec</a></li>
</ul>
</ul>
<p>The assistant will classify whether an API needs to be called based on the user's query, construct the API call and then summarize the response back to the user.</p>

<h2>RAG example: learning knowledge</h2>

<pre><code class="language-yaml">name: Printer advice tool
description: Ask me questions about printers
assistants:
- model: llama3:instruct
  rag_source_id: af9b4dc8-cc10-4e21-9a5f-bc1f12b6cab9
</code></pre>

<p style="float:right;"><div class="embed" style="width:360px; float:right;">
<script>
  ChatWidget({
    url: 'https://app.tryhelix.ai/v1/chat/completions',
    model: 'llama3:instruct',
    placeholder: "Inkjet versus laser printers?",
    bearerToken: 'hl-wRTXGBKGnMCXseyXb1VLnmVJ01Gesfn-H1Kh6tHrwHY=',
  })
</script>
</div></p>

<br />
<p>New fields in this example (for a given assistant) are:</p>
<ul>
<li><strong>rag_source_id</strong>: the ID of a RAG source in a compliant tool's vector database. In this case we added this <a href="https://www.connection.com/content/buying-guide/printer">printer buying guide</a></li>
</ul>
<p>The assistant will search the vector database for relevant content to the user's query and include it in the prompt to the model, which will then have context to share with the user. This can be used to "teach" models about documents.</p>


<h2>Fine-tuning example</h2>

<pre><code class="language-yaml">name: Talk like Paul Graham
description: Generate content in the style of Paul Graham's essays
assistants:
- model: llama3:instruct
  lora_id: TODO from https://app.tryhelix.ai/session/ses_01j0n113spfdpabftychs6g7ct
</code></pre>
<p style="text-align:right;"><a href="https://app.tryhelix.ai/new?app_id=app_01j0ke73c703awdmjghmmb059g" target="_blank" class="button">Run</a></p>
<p>New fields in this example (for a given assistant) are:</p>
<ul>
<li><strong>lora_id</strong>: the ID of a fine-tuned model file in a compliant tool's store</li>
</ul>
<p>The LoRA adapter is the result of additional training (known as "fine tuning") on new data. The assistant will load the LoRA adapter (a "diff" on the weights of the model) which allows the model to learn style and knowledge from the content. The model will be able to replicate the style and knowledge in the data it was fine-tuned on.</p>


<h2>GPTScript</h2>

<pre><code class="language-yaml">name: Generate recipe recommendations
description: For a given customer, recommend recipes based on their purchase history
assistants:
- model: llama3:instruct
  gptscripts:
   - file: scripts/waitrose.gpt
</code></pre>

<pre><code class="language-text">name: Generate recipe recommendations
tools: recipe.query, purchases.query, sys.read
args: user_id: The user_id we want to know about.
args: recipe_theme: The theme of the recipes.
args: number: The number of recipes to output.

Do the following steps sequentially and only write and read files as instructed:
  1. Run tool {recipe.query} to get a list of candidate recipes for the given
     user as a CSV file written to recipes.csv.
  2. Run tool {purchases.query} to get a list of the top 10 products most
     bought by the given user written to purchases.csv.
  3. Read files recipes.csv (the suggested recipes) and purchases.csv (the
     user's previous top purchase history) and output a JSON list of {number},
     {recipe_theme} theme recipes that you think the user would like based on
     their purchase history.

Format the final output in a strict JSON format.
Format the output to display recipe details including name, summary, and image URL.
In the summary, justify to the user why they would like the recipe.

For example, say in the summary (do NOT include parts in square brackets) "We
thought you would like this recipe because you have previously bought cod and
potatoes [previously purchased products]. It matches heart healthy [chosen
theme] because [insert justification based on nutritional information]"

Only include previously purchased products that appear in the recipe.

Output the exact image url from the CSV file, do not invent one. Output format:

[{
  "recipe.name": "name",
  "recipe.summary": "summary",
  "recipe.imageurl": "imageurl"
}]
</code></pre>

<p style="text-align:right;"><a href="https://app.tryhelix.ai/new?app_id=app_01j0ke73c703awdmjghmmb059g" target="_blank" class="button">Run</a></p>
<p>New fields in this example (for a given assistant) are:</p>
<ul>
<li><strong>lora_id</strong>: the ID of a fine-tuned model file in a compliant tool's store</li>
</ul>
<p>The LoRA adapter is the result of additional training (known as "fine tuning") on new data. The assistant will load the LoRA adapter (a "diff" on the weights of the model) which allows the model to learn style and knowledge from the content. The model will be able to replicate the style and knowledge in the data it was fine-tuned on.</p>


<h1>Interface layer</h1>

<h2>UI interface</h2>
<h2>Embeds</h2>
<h2>OpenAI compatible API</h2>
<h2>Language-specific SDKs</h2>
<h2>Voice</h2>

<h1>Version controlled configuration (GitOps)</h1>

<h1>Kubernetes Integration (CRDs)</h1>

<p>Take a 

<h1>AI-spec Compliant Tooling</h1>
The following tools support at least a subset of the AI Spec.
<ul>
<li><a href="https://helix.ml">HelixML</a></li>
</ul>
<h1>Possible future improvements</h1>
<ul>
<li>The <strong>model</strong> field could support more formats than just Ollama. We could update the format to <code>ollama://llama3:instruct</code> instead of just <code>llama3:instruct</code> to make room for other ways to reference models (e.g. huggingface format).</li>
<li>Input and output types other than just text, e.g. images.</li>
<li>New objects to declaratively define "data entities" such as RAG sources and fine-tuned LoRAs, rather than just referencing them by ID.</li>
</ul>

<br />
<hr />
<p>Created by <a href="https://helix.ml">HelixML</a>. Other organizations welcome to contribute, join the <code>#aispec</code> channel on <a href="https://mlops.community">MLOps.Community Slack</a>.</p>
</body>
